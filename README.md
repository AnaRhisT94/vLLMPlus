The code in this repo was used to produce [How continuous batching enables 23x throughput in LLM inference while reducing p50 latency](https://www.anyscale.com/blog) (TODO link to correct blog post).
